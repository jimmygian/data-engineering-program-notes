This is IBM's and AWS's official Data Engineering Certificate Program, comprising 4 courses:

* Course 1: Introduction to Data Engineering
* Course 2: Source Systems, Data Ingestion, and Pipelines
* Course 3: Data Storage and Queries
* Course 4: Data Modeling, Transformation, and Serving

___


>Fundamentals of Data Engineering (BOOK) - Resource
>https://www.redpanda.com/guides/fundamentals-of-data-engineering

Data Centric AI: Data-centric, as you all know, is the discipline of systematically entering the data, to build a successful AI system. 

This program is all about framework, principles, getting you to think like a data engineer. 

This program comprises four courses. 

* In this first course, we'll be looking at the big picture, forming a mental framework for data engineering and spinning up some Cloud data pipelines. 

* In the second course, we'll focus on ingestion of data from source systems, data ops, and orchestration of your data pipelines. 

* The third course is all about data storage in the Cloud, and how storage is an essential part of all stages of the data engineering life cycle. 

* The fourth course is about modeling, transforming, and serving data for END-USE cases. There's a lot to cover, and I'm looking forward to sharing all this with you. All that sounds fantastic. Let's go on to the next video and get started. 

## **What background knowledge do I need before taking this course?**

- **Intermediate Python programming skills** are required, including familiarity with Python syntax, data structures, functions, and classes. Some familiarity with Pandas dataframes may also be helpful but is not required. To learn the basics of working with Pandas dataframes, we recommend the [W3 School Pandas tutorials](https://www.w3schools.com/python/pandas/default.asp) or the [Kaggle Pandas tutorials](https://www.kaggle.com/learn/pandas).
    
- **Basic familiarity with SQL** may also helpful but not required. Feel free to check out the [SQLBolt Tutorials course](https://sqlbolt.com/) if you want to learn the basics of SQL.
    
- **Basic familiarity with the technical fundamentals of the AWS cloud** will be helpful but not required. To learn the basics of AWS we recommend the [AWS Cloud Practitioner Essentials](https://www.coursera.org/learn/aws-cloud-practitioner-essentials) and [AWS Cloud Technical Essentials](https://www.coursera.org/learn/aws-cloud-technical-essentials) courses.


# **Program Outline**

This program is structured as 4 courses.

[**Course 1 - Introduction to Data Engineering**](https://www.coursera.org/learn/intro-to-data-engineering/home/welcome)

This course consists of 4 weeks of content and covers these main learning objectives:

- Identify key upstream and downstream collaborators and stakeholders for data engineers
- Articulate a mental framework for building data engineering solutions
- Identify some of the necessary considerations for requirements gathering at the start of a new project
- Describe the structure of the data engineering lifecycle and its undercurrents, and how to think about data engineering problems through this lens
- Identify some of the key technologies that can be employed in different stages of the data engineering lifecycle
- Evaluate technologies and tools against the context of requirements and good data architecture
- Design a data architecture on AWS based on stakeholder requirements
- Implement a batch and streaming pipeline on AWS to support a product recommendation system


[**Course 2 - Source Systems, Data Ingestion, and Pipelines**](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/home/welcome)

This course consists of 4 weeks of content and covers these main learning objectives:

- Identify different data formats and determine appropriate source systems for generating each type of data
- Explain at a high level how data is generated, stored, and retrieved in various source systems, including relational databases, NoSQL databases, object storage, and streaming systems
- Explain the basics of cloud networking
- Troubleshoot database connection errors
- Explain the difference between batch and streaming ingestions and identify uses cases for each pattern
- Differentiate between the two batch ingestion patterns: Extract-Transform-Load (ETL) and Extract-Load-Transform (ELT)
- Create a script to ingest data from a REST API
- Describe the basic components of an event-streaming platform
- Interact with an event streaming platform as a source system and as an ingestion tool
- Use Terraform to provision AWS resources for your data pipeline
- Identify tools for monitoring your data systems and data quality
- Identify and monitor relevant data quality metrics
- Explain how orchestration can be applied to a data pipeline, and list its benefits
- Build data pipelines with DAGs in Airflow using features such as Taskflow API, operators, XCom variables, etc.

[**Course 3 - Data Storage and Queries**](https://www.coursera.org/learn/data-storage-and-queries/home/welcome)

This course consists of 3 weeks of content and covers these main learning objectives:

- Explain how data is physically stored on disk and in memory
- Compare how data is stored and queried in object, block, and file storage systems
- Explain how data is stored in row-oriented vs column-oriented databases
- Explain how graph and vector databases store and retrieve data
- Explain the key architectural features of data warehouses, data lakes, and data lakehouses
- Implement a data lake using AWS Glue
- Implement a data lakehouse with a medallion-like architecture using Lake Formation and Iceberg
- Explain the stages of the life of a query
- Implement advanced SQL queries
- Explain the role of of an index and its impact on query performance
- Summarize approaches for processing aggregate and join queries
- Compare the execution times of aggregate queries between row and columnar storage
- List some strategies for enhancing query performance
- Aggregate and join streaming data

[**Course 4 - Data Modeling, Transformation, and Serving**](https://www.coursera.org/learn/data-modeling-transformation-serving/home/welcome)

This course consists of 4 weeks of content and covers these main learning objectives:

- Define data modeling and its role in reflecting business logic
- Apply the normalization stages to a denormalized table
- Describe the fact and dimension tables of a star schema and transform data in third normal form to a star schema
- Describe the data warehouse modeling approaches such as Inmon, Kimball, Data Vault, and One Big Table
- Use feature engineering to convert a dataset into a tabular form thatâ€™s expected by a classical machine learning algorithms
- Preprocess and vectorize textual data
- List techniques for processing and augmenting image data
- Compare an in-memory processing framework like Spark, and a disk-based processing framework like Hadoop
- Describe the technical considerations for choosing a distributed processing framework, such as Spark, vs a non-distributed framework like Pandas dataframes
- Describe the technical considerations for using Spark SQL vs Spark DataFrames when transforming data using PySpark
- Describe how streaming transformation works with a near-real time processing engine such as Spark Structured Streaming
- Identify different ways of serving data for analytics and machine learning use cases
- Describe the purpose of a semantic layer that sits on top of the data model
- Create views and materialized views
- Describe the benefits and drawbacks of serving data using views and materialized views

